swift llm


high-performance inference engine for running LLM's

- Optimized memory management and pruning and LLM inference
- Efficient streaming and batching
- Simple API for runningh models
- Minimal overhead, focused on speed.

features:
- Smart memrory allocation optimized for LLM ops
- KV-Cache prioritization and management
- Dynamic batch scheduling
- Low latency response streaming
- Easy integration with existing models

This isn't a model implementation, a tiny LLM, or a model training library :)


